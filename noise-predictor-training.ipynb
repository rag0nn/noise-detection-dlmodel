{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8c2e74-34f8-4c56-b54d-8c0c1d299592",
   "metadata": {},
   "source": [
    "Training of model which has prediction noise in the image ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b100c98-f24d-4d91-bfc9-3afc7afefdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,BatchNormalization,Dropout,Conv2D,Conv2DTranspose,Activation,Flatten,LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from IPython.display import clear_output\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f64e51a-64c2-4ba1-978e-06f16285c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 1200\n",
    "IMAGE_HEIGHT = 600\n",
    "IMAGE_CHANNEL = 3\n",
    "BATCH_SIZE = 4\n",
    "SEGMENT_PER_IMAGE = 4\n",
    "EPOCHS = 100\n",
    "AUTO_SAVE_PERIOT = 1\n",
    "TRAIN_TOPIC = datetime.now().strftime(\"%m_%d-%H_%M\")\n",
    "\n",
    "DATA_LIMIT = 200\n",
    "IMAGES_PATH = './images_1'\n",
    "TEST_IMAGES_PATH = './images_2'\n",
    "global IMAGE_WIDTH,IMAGE_HEIGHT,IMAGE_CHANNEL,BATCH_SIZE,EPOCHS,IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79fc620-c756-4658-a462-5d6d0521666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20547f39-4885-48eb-9308-60496e6c5304",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a55876-215d-4f72-af49-49bef69e1391",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09866266-3946-43fc-ab48-7a431a798ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_loud(image,min_mean=-150,max_mean=150,max_sigma=300):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        image: Image will add a noise\n",
    "        min_mean: minimum random limit of noise mean\n",
    "        max_mean: maximum random limit of noise mean\n",
    "        max_sigma: maximum random limit of max standart deviation [0:max_sigma]\n",
    "    return:\n",
    "        noisy_image: image which noise added\n",
    "        noise_image: image of noise mask\n",
    "    \"\"\"\n",
    "    mean = np.random.randint(min_mean,max_mean)\n",
    "    sigma = np.random.randint(0,max_sigma)\n",
    "    \n",
    "    gaussian_noise = np.random.normal(0, 150, image.shape).astype(np.float32)\n",
    "    \n",
    "    noisy_im = cv2.add(image.astype(np.float32),gaussian_noise)\n",
    "    \n",
    "    higher_mask = np.ones_like(image) * 255 \n",
    "    higher_map = np.clip(cv2.subtract(noisy_im,higher_mask.astype(np.float32)),0,1500)\n",
    "    lower_map = np.clip(noisy_im,-1500,0)\n",
    "    \n",
    "    absolute_gaussian_noise = cv2.add(higher_map,lower_map)\n",
    "    \n",
    "    absolute_noisy_im = cv2.add(image.astype(np.float32),absolute_gaussian_noise)\n",
    "    return absolute_noisy_im,absolute_gaussian_noise\n",
    "    \n",
    "def scale_im(im):\n",
    "    im = im / 255\n",
    "    return im\n",
    "\n",
    "def descale_im(im):\n",
    "    im = im * 255\n",
    "    im = im.astype(np.uint8)\n",
    "\n",
    "    min_val = im.min()\n",
    "    max_val = im.max()\n",
    "\n",
    "    if min_val < 0 or max_val > 255:    \n",
    "        # 0 ile 255 arasında ölçekleme yap\n",
    "        im = ((im - min_val) / (max_val - min_val) * 255).astype('uint8')\n",
    "        \n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ce4f1-be3c-41c7-a7fc-ee88c0c5f74c",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc023de6-d9cd-437a-86bf-ce86fd55fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET SIZE: 52.53 MB\n",
      "TOTAL DATA COUNT: 200\n"
     ]
    }
   ],
   "source": [
    "## total data file size\n",
    "total = 0\n",
    "TOTAL_DATA_COUNT = 0\n",
    "for i in os.listdir(IMAGES_PATH)[:DATA_LIMIT]:\n",
    "    path = f\"{IMAGES_PATH}/{i}\"\n",
    "    size = os.path.getsize(path)\n",
    "    total += size\n",
    "    TOTAL_DATA_COUNT +=1\n",
    "\n",
    "print(f\"DATASET SIZE: {total / (1024 * 1024):.2f} MB\")  # Megabyte\n",
    "print(f\"TOTAL DATA COUNT: {TOTAL_DATA_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fb0c20-b150-4d81-8a8a-9a9fb643e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_im(im, sub_im_width=1200, sub_im_height=600):\n",
    "    shape = tf.shape(im)  \n",
    "    im_height, im_width = shape[0], shape[1]\n",
    "\n",
    "    top_left = im[0:sub_im_height, 0:sub_im_width]    \n",
    "    top_right = im[0:sub_im_height, im_width-sub_im_width:im_width]    \n",
    "    bottom_left = im[im_height-sub_im_height:im_height, 0:sub_im_width]    \n",
    "    bottom_right = im[im_height-sub_im_height:im_height, im_width-sub_im_width:im_width]   \n",
    "\n",
    "    return tf.stack([top_left, top_right, bottom_left, bottom_right])\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    def _read_image(image_path):\n",
    "        try:\n",
    "            image = tf.io.read_file(image_path)\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "        except Exception as e:\n",
    "            print(f\"Hata: {image_path} dosyasında sorun var -> {e}\")\n",
    "        return segment_im(image) \n",
    "        \n",
    "    names = os.listdir(IMAGES_PATH)[:DATA_LIMIT]\n",
    "    image_paths = list(f\"{IMAGES_PATH}/{name}\" for name in names)\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(lambda x: _read_image(x))\n",
    "    dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))  # Alt görselleri aç\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    for image in dataset.take(1):\n",
    "        print(image.shape)  # (batch_size, sub_im_height, sub_im_width, 3) olmalı\n",
    "\n",
    "    test_im = cv2.imread(image_paths[0])\n",
    "    return dataset,test_im\n",
    "\n",
    "def load_noisy_dataset():\n",
    "    def _read_image(image_path):\n",
    "        try:\n",
    "            image = tf.io.read_file(image_path)\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "        except Exception as e:\n",
    "            print(f\"Hata: {image_path} dosyasında sorun var -> {e}\")\n",
    "        return segment_im(image) \n",
    "        \n",
    "    names = os.listdir(IMAGES_PATH)[:DATA_LIMIT]\n",
    "    image_paths = list(f\"{IMAGES_PATH}/{name}\" for name in names)\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(lambda x: _read_image(x))\n",
    "    dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))  # Alt görselleri aç\n",
    "    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    for image in dataset.take(1):\n",
    "        print(image.shape)  # (batch_size, sub_im_height, sub_im_width, 3) olmalı\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95d2eb-b7ba-4800-a072-6c6769ea8a96",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9fbe80-0737-42f7-a188-e863fd855730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"noisier_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 600, 1200, 3)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 300, 600, 24)      1824      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 300, 600, 24)     96        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300, 600, 24)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 150, 300, 24)      5208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 150, 300, 24)     96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 150, 300, 24)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 75, 150, 24)       5208      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 75, 150, 24)      96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 75, 150, 24)       0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 150, 300, 24)     5208      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 150, 300, 24)     96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 150, 300, 24)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 300, 600, 24)     5208      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 300, 600, 24)     96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 300, 600, 24)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 600, 1200, 3)     1803      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,939\n",
      "Trainable params: 24,699\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    inp = Input(shape=(IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNEL))\n",
    "\n",
    "    x = Conv2D(24,(5,5),(2,2),padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(24,(3,3),(2,2),padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(24,(3,3),(2,2),padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2DTranspose(24,(3,3),(2,2),padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2DTranspose(24,(3,3),(2,2),padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    out = Conv2DTranspose(3,(5,5),(2,2),padding='same',activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inp,out,name='noisier_model')\n",
    "\n",
    "opt = Adam(learning_rate=0.0002,beta_1=0.5)\n",
    "model = create_model()\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982aae0-af80-472e-9285-900827238158",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8286e11e-cce5-484b-864e-4f126f0faee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch,model,clear_loss,clear_acc,noisy_loss,noisy_acc,interval_test_sample):\n",
    "\n",
    "    model.save(f'./training/noise-predictor/{TRAIN_TOPIC}_{epoch}.h5')\n",
    "    \n",
    "    # metrics save\n",
    "    f = open('./training/noise-predictor-checkpoint.txt','a')\n",
    "    f.writelines(f\"\\ntrain:{TRAIN_TOPIC} epoch:{epoch} clear_loss:{clear_loss:.7f} noisy_loss:{noisy_loss:.7f} clear_acc:{clear_acc:.7f} noisy_acc:{noisy_acc}\")\n",
    "    f.close()\n",
    "\n",
    "    # sample save\n",
    "    cv2.imwrite(f'./training/samples/{TRAIN_TOPIC}_{epoch}.jpg',interval_test_sample)\n",
    "\n",
    "def interval_test(model,orig_im):\n",
    "    orig_im = scale_im(orig_im)\n",
    "    noisy_im,noise = add_loud(orig_im)\n",
    "    noisy_im = scale_im(noisy_im)\n",
    "    orig_est =model.predict(orig_im)\n",
    "    noisy_est = model.predict(noisy_im)\n",
    "\n",
    "    orig_im = descale_im(orig_im)\n",
    "    orig_est = descale_im(orig_est)\n",
    "    noisy_im = descale_im(noisy_im)\n",
    "    noisy_est = descale_im(noisy_est)\n",
    "    \n",
    "    sample = np.zeros((IMAGE_HEIGHT*2,IMAGE_WIDTH*2,3), dtype=np.uint8)\n",
    "    sample[0:IMAGE_HEIGHT,0:IMAGE_WIDTH] = orig_im\n",
    "    sample[0:IMAGE_HEIGHT,IMAGE_WIDTH:IMAGE_WIDTH*2] = noisy_im\n",
    "    sample[IMAGE_HEIGHT:IMAGE_HEIGHT*2,0:IMAGE_WIDTH] =cv2.subtract(orig_im,orig_est).astype(np.uint8)\n",
    "    sample[IMAGE_HEIGHT:IMAGE_HEIGHT*2,IMAGE_WIDTH:IMAGE_WIDTH*2] = cv2.subtract(noisy_im,noisy_est).astype(np.uint8)\n",
    "\n",
    "    return sample\n",
    "\n",
    "def show_outputs(model,clear_losses,clear_accs,noisy_losses,noisy_acces,interval_test_sample):\n",
    "    epochs_history = range(len(clear_losses))\n",
    "\n",
    "    plt.figure(figsize=(10,7))    \n",
    "    plt.imshow(cv2.cvtColor(interval_test_sample,cv2.COLOR_BGR2RGB)); plt.axis(\"off\");plt.title('Test')\n",
    "    plt.show()\n",
    "\n",
    "    # Loss grafiği\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_history, clear_losses, label='Clear losses', color='blue', marker='o')\n",
    "    plt.plot(epochs_history, noisy_losses, label='Noisy losses', color='green', marker='s')\n",
    "\n",
    "    # Grafik ayarları\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Acc grafiği\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_history, clear_accs, label='Clear losses', color='blue', marker='o')\n",
    "    plt.plot(epochs_history, noisy_acces, label='Noisy losses', color='green', marker='s')\n",
    "\n",
    "    # Grafik ayarları\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b2ce4b7-1439-4ba9-9119-4f5a0fa281ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataset,test_im):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    clear_losses,clear_accs,noisy_losses,noisy_accs = [],[],[],[]\n",
    "    for i in tqdm.tqdm(range(EPOCHS)):\n",
    "        ep_start_time = time.time()\n",
    "\n",
    "        progress_bar = tqdm.tqdm(dataset,total=TOTAL_DATA_COUNT)\n",
    "\n",
    "\n",
    "        ep_clear_loss,ep_clear_acc,ep_noisy_loss,ep_noisy_acc = 0,0,0,0\n",
    "        counter = 0\n",
    "        for batch in progress_bar:\n",
    "            # data conjugations\n",
    "            noisy_image_batch = []\n",
    "            noise_batch = []\n",
    "            for image in batch:\n",
    "                noisy_image,noise = add_loud(image.numpy())\n",
    "                noisy_image_batch.append(scale_im(noisy_image).astype(np.float32))\n",
    "                noise_batch.append(scale_im(noise).astype(np.float32))\n",
    "            # İşlemleri tamamladıktan sonra yeniden TensorFlow tensörüne dönüştür\n",
    "            noisy_image_batch = tf.convert_to_tensor(np.array(noisy_image_batch))\n",
    "            noise_batch = tf.convert_to_tensor(np.array(noise_batch))\n",
    "\n",
    "            \n",
    "            # Son olarak batch'teki her bir resmi scale et\n",
    "            for i, im in enumerate(batch):\n",
    "                batch[i] = tf.convert_to_tensor(scale_im(im.numpy()))\n",
    "            #training\n",
    "            clear_loss,clear_acc = model.train_on_batch(noisy_image_batch,noise_batch)\n",
    "            noisy_loss,noisy_acc = model.train_on_batch(batch,np.zeros_like(batch.numpy().shape).astype(np.float32))\n",
    "\n",
    "            #metrics\n",
    "            ep_clear_loss += clear_loss\n",
    "            ep_clear_acc += clear_acc\n",
    "            ep_noisy_loss += noisy_loss\n",
    "            ep_noisy_acc += noisy_acc\n",
    "            counter += 1\n",
    "\n",
    "        ep_clear_acc / counter\n",
    "        ep_clear_loss / counter\n",
    "        ep_noisy_acc / counter\n",
    "        ep_noisy_loss / counter\n",
    "        clear_losses.append(ep_clear_loss)\n",
    "        clear_accs.append(ep_clear_acc)\n",
    "        noisy_losses.append(ep_noisy_loss)\n",
    "        noisy_accs.append(ep_noisy_acc)\n",
    "        \n",
    "        # Bellek temizliği\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "        clear_output()\n",
    "        interval_test_sample = interval_test(model,test_im)\n",
    "        show_outputs(model,clear_losses,clear_accs,noisy_losses,noisy_accs,interval_test_sample) \n",
    "\n",
    "        print(\"EPOCH:\",i)\n",
    "        print(\"MODEL_CLEAR,LOSS - ACC:\",ep_clear_acc,\" \",ep_clear_loss)\n",
    "        print(\"MODEL_NOISY,LOSS - ACC:\",ep_noisy_acc,\" \",ep_noisy_loss)\n",
    "        print(\"USED VRAM MEMORY: \",tf.config.experimental.get_memory_info('GPU:0'))  # VRAM bilgisi\n",
    "        print(\"ELAPSED TIME: \",time.time()-ep_start_time)\n",
    "        # save\n",
    "        if i % AUTO_SAVE_PERIOT == 0:\n",
    "            save_checkpoint(counter,model,ep_clear_loss,ep_clear_acc,ep_noisy_loss,ep_noisy_acc,interval_test_sample)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90562b0d-3195-4275-b7de-0a0681510378",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27fc6f09-330a-4e08-911e-140595bc9e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 600, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset,test_data= load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eaedea7-8a78-460d-b5c8-0ee4d46d18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|                                                                                          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, test_im)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Son olarak batch'teki her bir resmi scale et\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, im \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m---> 28\u001b[0m     batch[i] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(scale_im(im\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[0;32m     30\u001b[0m clear_loss,clear_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_on_batch(noisy_image_batch,noise_batch)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "train(model,dataset,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c717c9b-596d-4d74-922f-d7c698bef1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed683eb0-f865-4ef7-af70-f734f02c9a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
